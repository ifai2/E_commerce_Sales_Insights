{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## import libraries\n",
        "!pip install pyngrok streamlit\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "## Load Data\n",
        "customers = pd.read_csv('olist_customers_dataset.csv')\n",
        "geolocation = pd.read_csv('olist_geolocation_dataset.csv')\n",
        "order_items = pd.read_csv('olist_order_items_dataset.csv')\n",
        "order_payments = pd.read_csv('olist_order_payments_dataset.csv')\n",
        "order_reviews = pd.read_csv('olist_order_reviews_dataset.csv')\n",
        "orders = pd.read_csv('olist_orders_dataset.csv')\n",
        "products = pd.read_csv('olist_products_dataset.csv')\n",
        "sellers = pd.read_csv('olist_sellers_dataset.csv')\n",
        "category_translation = pd.read_csv('product_category_name_translation.csv')\n",
        "\n",
        "print(\"All datasets loaded successfully\")\n",
        "\n",
        "## Merge datasets\n",
        "def prepare_seller_data():\n",
        "    # Merge orders with payments to get actual revenue\n",
        "    orders_payments = orders.merge(\n",
        "        order_payments[['order_id', 'payment_value']],\n",
        "        on='order_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Merge with order items to get seller information\n",
        "    orders_items_payments = order_items.merge(\n",
        "        orders_payments[['order_id', 'order_purchase_timestamp', 'payment_value']],\n",
        "        on='order_id',\n",
        "        how='left'\n",
        "    ).merge(\n",
        "        order_reviews[['order_id', 'review_score']],\n",
        "        on='order_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Calculate seller statistics\n",
        "    seller_stats = orders_items_payments.groupby('seller_id').agg({\n",
        "        'order_id': 'nunique',\n",
        "        'order_item_id': 'count',\n",
        "        'price': 'mean',\n",
        "        'freight_value': 'mean',\n",
        "        'payment_value': 'sum',\n",
        "        'review_score': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    seller_stats.columns = [\n",
        "        'seller_id', 'total_orders', 'total_products_sold',\n",
        "        'avg_price', 'avg_freight', 'total_revenue', 'avg_review_score'\n",
        "    ]\n",
        "\n",
        "    # Merge with seller location data\n",
        "    seller_data = seller_stats.merge(sellers, on='seller_id', how='left')\n",
        "\n",
        "    # Handle missing values\n",
        "    seller_data.fillna({\n",
        "        'avg_review_score': seller_data['avg_review_score'].mean(),\n",
        "        'avg_price': seller_data['avg_price'].mean(),\n",
        "        'avg_freight': seller_data['avg_freight'].mean(),\n",
        "        'total_revenue': 0\n",
        "    }, inplace=True)\n",
        "\n",
        "    return seller_data\n",
        "\n",
        "seller_data = prepare_seller_data()\n",
        "print(\"Seller data prepared successfully\")\n",
        "\n",
        "## Create target variable\n",
        "median_revenue = seller_data['total_revenue'].median()\n",
        "seller_data['profitable'] = (seller_data['total_revenue'] > median_revenue).astype(int)\n",
        "print(\"Target variable created:\")\n",
        "print(seller_data['profitable'].value_counts())\n",
        "\n",
        "## Select features and split data\n",
        "features = ['total_orders', 'total_products_sold', 'avg_price', 'avg_freight', 'avg_review_score']\n",
        "X = seller_data[features]\n",
        "y = seller_data['profitable']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "## Train model\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        n_estimators=150,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "## Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\n Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Cross validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
        "\n",
        "# Save model and data\n",
        "with open('seller_profitability_model_simple.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Save complete seller data for Streamlit\n",
        "seller_data.to_csv('cleaned_seller_data.csv', index=False)\n",
        "\n",
        "print(\"Model and data saved successfully\")\n",
        "\n",
        "## Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Feature importance\n",
        "rf = model.named_steps['clf']\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=True)\n",
        "\n",
        "axes[0].barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "axes[0].set_title('Feature Importance')\n",
        "axes[0].set_xlabel('Importance')\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, ax=axes[1], cmap='Blues')\n",
        "axes[1].set_title('Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Additional analysis\n",
        "print(\"\\n Additional Analysis:\")\n",
        "print(f\"Total sellers: {len(seller_data)}\")\n",
        "print(f\"Profitable sellers: {seller_data['profitable'].sum()} ({seller_data['profitable'].mean()*100:.1f}%)\")\n",
        "print(f\"Median revenue: BRL {median_revenue:,.2f}\")\n",
        "\n",
        "# Show top features\n",
        "print(\"\\n Top Features by Importance:\")\n",
        "for feature, importance in zip(features, importances):\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load model and data\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    with open(\"seller_profitability_model_simple.pkl\", \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "@st.cache_data\n",
        "def load_seller_data():\n",
        "    return pd.read_csv(\"cleaned_seller_data.csv\")\n",
        "\n",
        "model = load_model()\n",
        "seller_data = load_seller_data()\n",
        "\n",
        "# Streamlit UI\n",
        "st.set_page_config(page_title=\"Seller Profitability Predictor\", page_icon=\"ðŸ›’\", layout=\"centered\")\n",
        "st.title(\"Seller Profitability Prediction App\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "Predict if a seller is **profitable (1)** or **not profitable (0)**\n",
        "based on key performance indicators.\n",
        "\"\"\")\n",
        "\n",
        "# User input\n",
        "st.subheader(\"Enter Seller Data\")\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    total_orders = st.number_input(\"Total Orders\", min_value=0, step=1, value=50)\n",
        "    total_products_sold = st.number_input(\"Total Products Sold\", min_value=0, step=1, value=75)\n",
        "    avg_price = st.number_input(\"Average Price (BRL)\", min_value=0.0, step=1.0, value=120.0)\n",
        "\n",
        "with col2:\n",
        "    avg_freight = st.number_input(\"Average Freight (BRL)\", min_value=0.0, step=1.0, value=18.0)\n",
        "    avg_review_score = st.slider(\"Average Review Score\", 1.0, 5.0, 4.2, 0.1)\n",
        "\n",
        "# Prepare input\n",
        "input_df = pd.DataFrame([{\n",
        "    \"total_orders\": total_orders,\n",
        "    \"total_products_sold\": total_products_sold,\n",
        "    \"avg_price\": avg_price,\n",
        "    \"avg_freight\": avg_freight,\n",
        "    \"avg_review_score\": avg_review_score\n",
        "}])\n",
        "\n",
        "# Prediction\n",
        "if st.button(\"Predict Profitability\"):\n",
        "    prediction = model.predict(input_df)[0]\n",
        "    probability = model.predict_proba(input_df)[0]\n",
        "\n",
        "    st.subheader(\"Prediction Result\")\n",
        "    if prediction == 1:\n",
        "        st.success(f\"Profitable Seller (Confidence: {probability[1]:.1%})\")\n",
        "    else:\n",
        "        st.error(f\"Not Profitable Seller (Confidence: {probability[0]:.1%})\")\n",
        "\n",
        "    # Show probabilities\n",
        "    st.write(f\"Probability of being profitable: {probability[1]:.1%}\")\n",
        "    st.write(f\"Probability of not being profitable: {probability[0]:.1%}\")\n",
        "\n",
        "# Feature importance\n",
        "if st.button(\"Show Feature Importance\"):\n",
        "    st.subheader(\"Feature Importance\")\n",
        "    rf = model.named_steps['clf']\n",
        "    importances = rf.feature_importances_\n",
        "    features = ['total_orders', 'total_products_sold', 'avg_price', 'avg_freight', 'avg_review_score']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    y_pos = np.arange(len(features))\n",
        "    ax.barh(y_pos, importances)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(features)\n",
        "    ax.set_xlabel('Importance')\n",
        "    ax.set_title('Feature Importance in Profitability Prediction')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# Seller recommendation\n",
        "if st.button(\"Find Similar Profitable Sellers\"):\n",
        "    prediction = model.predict(input_df)[0]\n",
        "\n",
        "    if prediction == 0:\n",
        "        st.info(\"Finding profitable sellers with similar characteristics...\")\n",
        "\n",
        "        # Find profitable sellers with similar features\n",
        "        profitable_sellers = seller_data[seller_data['profitable'] == 1].copy()\n",
        "\n",
        "        if not profitable_sellers.empty:\n",
        "            # Calculate similarity score (simplified)\n",
        "            for feature in features:\n",
        "                profitable_sellers[f'{feature}_diff'] = abs(\n",
        "                    profitable_sellers[feature] - input_df[feature].iloc[0]\n",
        "                )\n",
        "\n",
        "            profitable_sellers['total_diff'] = profitable_sellers[[f'{f}_diff' for f in features]].sum(axis=1)\n",
        "            similar_sellers = profitable_sellers.nsmallest(3, 'total_diff')\n",
        "\n",
        "            st.subheader(\"Recommended Profitable Sellers\")\n",
        "\n",
        "            for idx, seller in similar_sellers.iterrows():\n",
        "                st.write(f\"**Seller ID:** {seller['seller_id']}\")\n",
        "                st.write(f\"**Location:** {seller.get('seller_city', 'Unknown')}, {seller.get('seller_state', 'Unknown')}\")\n",
        "                st.write(f\"**Performance:** {int(seller['total_orders'])} orders, {seller['avg_review_score']:.1f}â˜… rating\")\n",
        "                st.write(f\"**Revenue:** BRL {seller['total_revenue']:,.2f}\")\n",
        "                st.write(\"---\")\n",
        "        else:\n",
        "            st.warning(\"No profitable sellers found in the database.\")\n",
        "    else:\n",
        "        st.success(\"Your seller is already profitable! ðŸ’¸\")\n",
        "\n",
        "st.sidebar.info(\"\"\"\n",
        "**About this app:**\n",
        "- Predicts seller profitability using machine learning\n",
        "- Trained on Olist e-commerce data\n",
        "- Uses Random Forest classifier\n",
        "- Accuracy: 94.7%\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "!pip install pyngrok streamlit\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def cleanup():\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "        os.system(\"pkill -f streamlit\")\n",
        "        os.system(\"pkill -f ngrok\")\n",
        "        os.system(\"fuser -k 8501/tcp 8502/tcp 8503/tcp 2>/dev/null\")\n",
        "        time.sleep(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Cleanup: {e}\")\n",
        "\n",
        "cleanup()\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"34ddysFwXISWWW6um42MVQKsnpD_4DGgG45UWFvCipQPRi5Jm\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "print(\"Ngrok authentication set successfully\")\n",
        "\n",
        "# Streamlit\n",
        "def start_streamlit_app(port=8501):\n",
        "    try:\n",
        "\n",
        "        os.system(f\"fuser -k {port}/tcp 2>/dev/null\")\n",
        "        time.sleep(2)\n",
        "\n",
        "        process = subprocess.Popen([\n",
        "            \"streamlit\", \"run\", \"app.py\",\n",
        "            \"--server.port\", str(port),\n",
        "            \"--server.headless\", \"true\",\n",
        "            \"--server.address\", \"0.0.0.0\"\n",
        "        ])\n",
        "\n",
        "        print(f\"Starting Streamlit on port {port}...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(f\"http://localhost:{port}\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"Streamlit is running on port {port}\")\n",
        "                return process, port\n",
        "        except:\n",
        "            print(f\"Streamlit failed to start on port {port}\")\n",
        "            process.terminate()\n",
        "            return None, port\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error starting Streamlit: {e}\")\n",
        "        return None, port\n",
        "\n",
        "ports_to_try = [8501, 8502, 8503, 8504]\n",
        "streamlit_process = None\n",
        "success_port = None\n",
        "\n",
        "for port in ports_to_try:\n",
        "    streamlit_process, success_port = start_streamlit_app(port)\n",
        "    if streamlit_process:\n",
        "        break\n",
        "\n",
        "if not streamlit_process:\n",
        "    print(\"Failed to start Streamlit on any port\")\n",
        "else:\n",
        "    print(f\"Streamlit running successfully on port {success_port}\")\n",
        "\n",
        "    # Ngrok pipeline\n",
        "    try:\n",
        "        print(\"ðŸ”— Creating ngrok tunnel...\")\n",
        "        public_url = ngrok.connect(success_port, bind_tls=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\" SUCCESS! Your App is Live! \")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Public URL: {public_url}\")\n",
        "        print(\"Share this link with anyone!\")\n",
        "        print(\"The app will auto-close when this session ends\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        tunnels = ngrok.get_tunnels()\n",
        "        print(f\" Active tunnels: {len(tunnels)}\")\n",
        "        for tunnel in tunnels:\n",
        "            print(f\"   - {tunnel.public_url} -> {tunnel.config['addr']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create ngrok tunnel: {e}\")\n",
        "        print(\"\\n Alternative solutions:\")\n",
        "        print(\"1. Wait 2 minutes and try again\")\n",
        "        print(\"2. Visit https://dashboard.ngrok.com/endpoints/status\")\n",
        "        print(\"3. Use this command: !ngrok http 8501\")\n"
      ],
      "metadata": {
        "id": "do_XSnYbGyD7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7vLuPwJHQAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}